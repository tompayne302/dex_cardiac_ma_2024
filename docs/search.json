[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "",
    "text": "# Overall forest plot\nBelow in Figure 3 and Figure 4 I show the forest plot of all studies, and excluding studies at high risk of bias, respectively.\nFor meta-analysis, we will employ a Bayesian normal-normal hierarchical model (NNHM). It is a ‘normal-normal’ model because we assume on the first level that the within-study effects of each trial are normally distributed, and on the second level that the effects across different populations are normally distributed.\nFirstly - let’s discuss the within-study normality assumption. In the NNHM we assume that each study’s observed effect \\(\\hat{\\theta}\\) is an estimate of the true effect in that trial population \\({\\theta}\\), and the uncertainty in this estimate is modeled with a normal distribution with mean \\({\\theta}\\) and standard deviation \\({\\sigma}\\):\n\\[\n\\hat{\\theta} \\sim Normal({\\theta}, {\\sigma}^2)\n\\]\nIn the above case, the standard deviation \\(\\sigma\\) is the trial’s standard error. As such, estimates from studies with smaller standard errors will have narrower normal distributions and deviate less from the estimated mean effect \\(\\theta\\). The purple lines in the forest plot below show the assumed distribution of within study effects described above.\nSecondly - let’s discuss the between-population normality assumption. Here, we assume that each study’s estimate \\(\\theta\\) is an approximation of the overall mean effect across different populations, \\(\\mu\\). We assume that this normal distribution with mean \\(\\mu\\) has a standard deviation \\(\\tau\\). So, the second level of our NNHM takes the form:\n\\[\n\\theta \\sim Normal(\\mu, \\tau^2)\n\\]\nIn a Bayesian analysis, we know \\(\\theta\\) using our estimate \\(\\hat{\\theta}\\), and we assume that \\(\\sigma\\) is known (for which use the study’s standard error). We need to put priors on both of these \\(\\mu\\) and \\(\\tau\\) parameters. For our primary analysis, I have used weakly informative priors. That is, they are meant to impart some information about prior belief of the plausible range of true values, without overwhelming the data.\nFor \\(\\mu\\), the prior is a normal distribution with mean 0 and standard deviation 0.82 (on the log odds ratio scale):\n\\[\n\\mu \\sim Normal(0, 0.82)\n\\]\nThis is a weakly informative prior because it makes values closer to null (logOR = 0) more likely but still leaves room for higher and lower values. With this prior, 95% of the density lies between an odds ratio of 0.2 and 5. This is consistent with logic, and prior evidence: we consider it unlikely that dexmedetomidine is a silver bullet for delirium (80% or greater odds reduction) or a significant driver of delirium (500% or greater odds increase).\nBelow in Figure 1, I graph the prior on both the logOR (A) and OR scale (B), to visualise the prior distribution. As you can see, for the distribution to be normal on the odds ratio scale, we need to take the logarithm of the x-axis (C).\nFigure 1: Plot of the prior for the mean effect, in A) log odds ratio form with continuous x-axis, B) odds ratio form with continuous x-axis, and C) odds ratio form with log10 x-axis.\nFor \\(\\tau\\), the prior is a half-Cauchy distribution with scale = 0.5:\n\\[\n\\tau \\sim HalfCauchy(0.5)\n\\]\nWe use a half-Cauchy distribution because this is a heavy tailed distribution that only takes positive values (as we only look at ‘half’), which is appropriate for a standard deviation parameter. The ‘scale’ in a half-Cauchy distribution can be thought of as similar to a standard deviation, as it relates to the gradient of the slope (this isn’t technically true because the Cauchy distribution has undefined mean and variance, but is okay for purposes of understanding).\nWe can show our \\(\\tau \\sim HalfCauchy(0.5)\\) prior graphically. Somewhat arbitrarily, traditional cutoffs for heterogeneity are: Low (\\(\\tau &lt; 0.1\\)), reasonable (\\(0.1 &lt; \\tau &lt; 0.5\\)), fairly high (\\(0.5 &lt; \\tau &lt; 1.0\\)), and fairly extreme (\\(\\tau &gt; 1.0\\)). We can present the prior probabilities of each level of heterogeneity graphically, as shown in Figure 2 below.\nAs you’ll notice, the distribution is heavy-tailed, and as such we leave a high probability for significant heterogeneity. This is consistent with our belief that there is likely quite significant heterogeneity in the response to dexmedetomidine among different populations.\nFigure 2: Plot of the prior for heterogeneity parameter tau, with probabilities of each level of heterogeneity shown.\nFinally, we will conduct metaregression further below, with more details provided. Nonetheless I will show the priors for the regression coefficient here, for completeness. For the regression coefficient we will use the same priors as we use for the intercept above: \\(\\beta \\sim Normal(0, 0.82)\\)\nPutting all these together, we can show the following formulas for our NNHM:\n\\[\\hat{\\theta} \\sim Normal(\\theta, \\sigma^2)\\] \\[\\theta \\sim Normal(\\mu, \\tau^2)\\] \\[\\mu \\sim Normal(0,0.82)\\] \\[\\tau \\sim HalfCauchy(0.5)\\] \\[\\beta \\sim Normal(0,0.82)\\]\nBefore we can run the analysis, we need to consider an important issue: two of the studies were registered after enrollment began (Chitnis et al., Soh et al.), meaning there is a possibility of bias. Rather than outright exclude these studies, it would be better to try and model the suspected bias (thanks to Prof James Brophy for this suggestion). To do this, we will run a stepwise procedure. Let’s start with the Chitnis study as an example.\nFirstly, the marginal effect size for Chitnis that we calculate is: OR = 0.43, 95%CI: 0.15 to 1.25. On the logOR scale, this is equivalent to logOR = -0.8341, SE: 0.5376802.\nAs explained above, in a normal-normal hierarchical model model meta-analysis, we assume that each study’s true effect size follows a normal distribution with mean = calculated log odds ratio and SD = calculated standard error. So, we can draw random samples from this assumed distribution using the code:\nset.seed(123)\n\neffect_dist &lt;- rnorm(1000, -0.8341, 0.5376802)\nhead(effect_dist)\n\n[1] -1.135456658 -0.957861879  0.003986598 -0.796189034 -0.764584545\n[6]  0.088056485\nBut these estimates need to be modified due to possible bias arising from registration after enrollment beginning. A study by Deschartres and colleagues showed that there approximately a 19% greater effect size in studies that were unregistered or retrospectively registered (combined ROR = 0.81, 95% CI 0.65–1.02, based on 32 contributing meta-analyses).\nBased on this effect size and confidence interval, we can model the ‘inflation’ component as a normal distribution with mean 0.82 (corresponding to a 1 - 0.81 = 19% increase in effect) and standard deviation = 0.1:\nset.seed(123)\n\ninflation_dist &lt;- rnorm(1000, 0.81, 0.1)\nWe can then multiply this bias adjustment by our assumed distribution of effect sizes from Chitnis, and take the mean and SD of the resulting distribution as the revised effect size and SE. We also need to add a for-loop because we’re on the logOR scale, with more negative values implying more benefit, so we need to be sure that ‘inflating’ the effect size means negative values are made less negative and positive values are made more positive. So, in cases where the value for inflation distribution is &lt;1 and the value for the effect size distribution is positive, then the reciprocal of ‘inflation’ should be used (e.g., 0.80 becomes 1.25). Similarly, if the value for inflation distribution is &gt;1 and the value for effect size distribution is positive, then the reciprocal of inflation should be used:\nset.seed(123)\n\ninflation_dist &lt;- rnorm(1000, 0.81, 0.1)\neffect_dist &lt;- rnorm(1000, -0.8341, 0.5376802)\n\nresult &lt;- numeric(length = 1000)\n\n# Loop through each pair of values\nfor (i in 1:1000) {\n    # Conditionally modify inflation value\n    if (inflation_dist[i] &lt; 1 && effect_dist[i] &gt; 0) {\n        inflation_adj &lt;- 1 / inflation_dist[i]\n    } else if (inflation_dist[i] &gt; 1 && effect_dist[i] &gt; 0) {\n        inflation_adj &lt;- 1 / inflation_dist[i]\n    } else {\n        inflation_adj &lt;- inflation_dist[i]\n    }\n\n    # Perform the multiplication\n    result[i] &lt;- inflation_adj * effect_dist[i]\n}\n\nmean(result)\n\n[1] -0.6478209\n\nsd(result)\n\n[1] 0.4599308\nAnd for Soh et al., the calculated OR is: OR = 0.26, 95%CI: 0.05 to 1.31. on the logOR scale, this is logOR = -1.347074, SE: 0.8266586.\nApplying the same procedure to Soh we get mean and SD:\nset.seed(123)\n\ninflation_dist &lt;- rnorm(1000, 0.81, 0.1)\neffect_dist &lt;- rnorm(1000, -1.347074, 0.8266586)\n\nresult &lt;- numeric(length = 1000)\n\n# Loop through each pair of values\nfor (i in 1:1000) {\n    # Conditionally modify inflation value\n    if (inflation_dist[i] &lt; 1 && effect_dist[i] &gt; 0) {\n        inflation_adj &lt;- 1 / inflation_dist[i]\n    } else if (inflation_dist[i] &gt; 1 && effect_dist[i] &gt; 0) {\n        inflation_adj &lt;- 1 / inflation_dist[i]\n    } else {\n        inflation_adj &lt;- inflation_dist[i]\n    }\n\n    # Perform the multiplication\n    result[i] &lt;- inflation_adj * effect_dist[i]\n}\n\nmean(result)\n\n[1] -1.049854\n\nsd(result)\n\n[1] 0.7047859\nNow for the forest plot.\nFirstly, the blue curves show the posterior shrinkage estimates of each study; that is, each study’s effect is ‘shrunk’ towards the mean when viewed in light of the other data. Kruschke and Liddell explain this best:\nThe orange line shows the 95%CrI for the prediction interval: the range of values that would be likely to be observed be a future study of this research question. Note this is different to the credible interval, which is the range of plausible values for the mean effect size. Prediction intervals are perhaps the best summary of heterogeneity because they provide a practical interpretation: what is the range of plausible values I might observe in different patient populations?\nOn the right hand side we have the risk of bias assessments for each study: the five domains, and the overall assessment. ‘+’ indicates ‘Low risk’, ‘-’ indicates ‘Some concerns’, and ‘x’ indicates ‘High risk’."
  },
  {
    "objectID": "index.html#explanatory-plots",
    "href": "index.html#explanatory-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "4.1 Explanatory plots",
    "text": "4.1 Explanatory plots\n\n\n\n\n\n\n\n\nFigure 5: Explanation of the fully Bayesian sequential method described by Spence et al. A) Shows when we stop for effect, B) when we stop for futility."
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "4.2 Analysis",
    "text": "4.2 Analysis\nSimilar to our model in brms, in rjags we need to specify priors for \\(\\mu\\) and \\(\\tau\\). However, JAGS uses precision rather than variance, where:\n\\[\nPrecision = \\frac{1}{Variance}\n\\]\nThis means our priors look different in rjags and JAGS, but they have the same result. When working with JAGS we are specifying a prior distribution for \\(\\tau^2\\), rather than \\(\\tau\\). It is difficult to specify a prior for \\(\\tau^2\\) that can closely mimic our earlier priors for \\(\\tau\\), and for this reason I have decided to specify a vague prior for \\(\\tau\\) in the sequential analysis model:\n\\[\n\\tau^2 \\sim HalfNormal(0, 10^2)\n\\]\nIn summary, our priors in JAGS are:\n\\[\\hat{\\theta} \\sim Normal(\\theta, \\frac{1}{\\sigma^2})\\] \\[\\theta \\sim Normal(\\mu, \\frac{1}{\\tau^2})\\] \\[\\mu \\sim Normal(0, \\frac{1}{0.82^2})\\] \\[\\tau^2 \\sim HalfNormal(0, \\frac{1}{10^2})\\]\nBelow in Table 3 we present the results of the fully Bayesian sequential analysis. As can be seen, this method suggests stopping for benefit after 15 studies when all studies are included, but the conclusion is uncertain when excluding studies at high risk of bias.\n\n\n\n\nTable 3: Fully Bayesian sequential method.\n\n\n\n\n\n\n\n\n\nSubgroup\nConclusion\nNumber of studies to reach conclusion\nTotal number of studies in subgroup\nMedian odds ratio (95%CrI) at stopping\nτ (95% CrI) at stopping\n\n\n\n\nAll studies\nUncertain\n12\n12\n0.65 (0.41, 0.95)\n0.53 (0.05, 1.03)\n\n\nExcluding studies at high RoB\nUncertain\n10\n10\n0.72 (0.47, 1.06)\n0.48 (0.01, 0.93)"
  },
  {
    "objectID": "index.html#semi-bayesian-sequential-analysis",
    "href": "index.html#semi-bayesian-sequential-analysis",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "5.3 Semi-Bayesian sequential analysis",
    "text": "5.3 Semi-Bayesian sequential analysis\nNow for our semi-Bayesian sequential analysis. This uses the methods of Higgins et al. This method is ‘semi-Bayesian’ because it used restricted Whitehead monitoring boundaries with an (approximate) Bayesian approach to updating heterogeneity, but \\({\\mu}\\) is still estimated using frequentist methods.\nIn the dataframe below, when \\(stopping = 0\\), the benefit is uncertain, but when \\(stopping = 1\\), the conclusion is benefit.\nThis method requires us to specify some inputs: \\({\\mu}_R\\), \\(H\\), and \\(V_{max}\\).\n\\({\\mu}_R\\) is the desired log odds ratio. For this, I have used the MCID of \\(logOR = 0.18\\), which I discussed above in Equation 5.\n\\(±H\\) is the horizontal boundaries of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(H\\) is set to 7.461.\n\\(V_{max}\\) is the maximum vertical boundary of the O’Brien-Fleming stopping boundary. To specify an \\({\\alpha}\\) of 0.05 and \\({\\beta}\\) of 0.1, \\(V_{max}\\) is set to 11.079.\nAs can be seen below in Table 5, this method suggests the benefit is uncertain with the current data. The slight discordance with the fully Bayesian model is more stringent criteria for stopping in the Semi-Bayesian model.\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\n\nTable 5:  Semi-Bayesian sequential method. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -1.7912194\n1.64957901\n-1.085865\n0.6062153\nNaN\nNaN\n2.7250232\n0\n    -1.2174181\n0.38609331\n-3.153171\n2.5900475\n-16.903946\n14.469110\n0.8258027\n0\n    -0.9770970\n0.22174684\n-4.406363\n4.5096471\n-9.989389\n8.035195\n0.5939973\n0\n    -0.8064147\n0.14883970\n-5.418008\n6.7186375\n-6.846852\n5.234022\n0.4760491\n0\n    -0.8299275\n0.11633039\n-7.134228\n8.5962057\n-5.558892\n3.899037\n0.3914527\n0\n    -0.6347392\n0.13756928\n-4.613961\n7.2690649\n-6.336986\n5.067507\n0.6852806\n0\n    -0.6904028\n0.11680912\n-5.910521\n8.5609756\n-5.454737\n4.073931\n0.6609892\n0\n    -0.5852817\n0.08414063\n-6.955993\n11.8848640\n-3.983478\n2.812915\n0.5202119\n0\n    -0.5847767\n0.07167924\n-8.158243\n13.9510411\n-3.495813\n2.326259\n0.4779245\n0\n    -0.5687122\n0.05858615\n-9.707280\n17.0688801\n-2.936798\n1.799374\n0.4218394\n0\n    -0.4989253\n0.04535299\n-11.000936\n22.0492627\n-2.319799\n1.321949\n0.3472847\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6:  Semi-Bayesian sequential method for this meta-analysis, excluding\nstudies at high risk of bias. \n  \n    \n    \n      y\n      var\n      Z\n      V\n      lower\n      upper\n      tausq\n      stopping\n    \n  \n  \n    NaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0\n    -0.7011444\n0.39925330\n-1.756139\n2.504676\nNaN\nNaN\n0.6666667\n0\n    -0.6339709\n0.22217420\n-2.853485\n4.500973\n-9.660082\n8.392140\n0.5000000\n0\n    -0.5530248\n0.14451299\n-3.826817\n6.919793\n-6.412056\n5.306007\n0.4000000\n0\n    -0.6234306\n0.11344903\n-5.495249\n8.814531\n-5.234851\n3.987989\n0.3333333\n0\n    -0.4032899\n0.12131915\n-3.324207\n8.242722\n-5.431969\n4.625389\n0.5274665\n0\n    -0.4968271\n0.10460251\n-4.749667\n9.560000\n-4.762609\n3.768955\n0.5243804\n0\n    -0.4276759\n0.07475448\n-5.721073\n13.377124\n-3.441101\n2.585750\n0.4143605\n0\n    -0.4354461\n0.06008578\n-7.247074\n16.642873\n-2.862698\n1.991805\n0.3664951\n0\n    -0.3824968\n0.04554934\n-8.397418\n21.954216\n-2.209317\n1.444323\n0.3008453\n0"
  },
  {
    "objectID": "index.html#cumulative-meta-analysis-plots",
    "href": "index.html#cumulative-meta-analysis-plots",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "5.4 Cumulative meta-analysis plots",
    "text": "5.4 Cumulative meta-analysis plots\nThe above analysis give good information but do not present the results succinctly to a reader. Therefore, below I’ve performed cumulative meta-analyses to show the results of the sequential analyses. These plots show the posterior of the pooled effect after adding the next study sequentially. As such, the confidence intervals should shrink with increasing studies towards the ‘true’ population mean. Figure 8 shows the findings.\n\n\n\n\n\nFigure 8: Cumulative forest plot."
  },
  {
    "objectID": "index.html#posterior-probabilities-with-different-priors",
    "href": "index.html#posterior-probabilities-with-different-priors",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.1 Posterior probabilities with different priors",
    "text": "3.1 Posterior probabilities with different priors\nFigure 5 shows the distributions of the priors. The top two panels show the distribution of the (A) reference priors, and (B) meta-analysis priors. The bottom forest plot shows the posterior distribution of DECADE’s results for each prior. The grey region denotes the ROPE. The purple lines show the DECADE’s findings when analysed alone. The grey lines represent the prior. Regions of the posterior that suggest benefit (OR &lt; 1) are shaded in blue and those that suggest harm (OR &gt; 1) are shaded in red. The probabilities of any harm or benefit, and the harm or benefit exceeding the MCID, are shown for each prior.\n\n\n\n\n\n\n\n\nFigure 5: Plots of the distributions of the reference and meta-analysis priors."
  },
  {
    "objectID": "index.html#arr-table",
    "href": "index.html#arr-table",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.3 ARR table",
    "text": "3.3 ARR table\nBelow we provide direct probability statements for the DECADE trial’s results given each priors we specified above.\nWe have chosen to present the risk of any harm or benefit, and the harm exceeding the MCID for benefit and harm (calculated by conversion of a logOR of +/-0.18 using the median control group rate of delirium in the DECADE trial).\nAll ARRs in Table 3 below use avg_comparisons() function from the marginaleffects package to obtain risk differences as the measure of effect. It does this via g-computation.\n\n\n\n\nTable 3: Explanations for priors for DECADE and the corresponding probabilities of changes in absolute risk of delirium.\n\n\n\n\n\n\n\n\n\n\nPosterior probability that the change in absolute risk from DECADE is above/below a certain threshold\n\n\nARR &gt;MCID for benefit (ARR = 2.3%)\nARR &gt;0%\nARI &gt;0%\nARI &gt;MCID for harm (ARI = 2.5%)\n\n\n\n\nReference priors\n\n\nVague\n0.1\n1.5\n98.5\n85.4\n\n\nSceptical\n0.0\n0.6\n99.4\n83.0\n\n\nNeutral\n0.2\n3.8\n96.2\n73.5\n\n\nOptimistic\n3.4\n27.3\n72.7\n18.8\n\n\nMeta-analysis priors\n\n\nMA (all studies)\n17.0\n63.7\n36.3\n3.0\n\n\nMA (low RoB)\n10.1\n46.2\n53.8\n9.5\n\n\nMA (bias adj.)\n0.2\n3.4\n96.6\n70.9"
  },
  {
    "objectID": "index.html#shrinkage-or-estimation-ecdfs",
    "href": "index.html#shrinkage-or-estimation-ecdfs",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.4 Shrinkage OR estimation + ECDFs",
    "text": "3.4 Shrinkage OR estimation + ECDFs\nNow let’s zoom on the relative distributions of the mean effect when we include and exclude the DECADE trial from the meta-analysis. For this we will use empirical cumulative distribution function (ECDF) plots.\nI consider four density functions: meta-analysis including DECADE, meta-analysis excluding DECADE, DECADE alone, and the DECADE shrinkage estimate from the meta-analysis.\nThe x-axis is common to the upper and lower graphs so one is able to read off the probability of any odds ratio for any of the four curves. Figure 6 shows the results. The grey region denotes the ROPE (\\(log(OR) = ± 0.18\\)).\n\nAll studiesExcluding studies at high risk of bias\n\n\n\n\n\n\n\n\n\n\nFigure 6: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Empirical cumulative distribution function (ECDF) plots including and excluding DECADE, and excluding studies at high risk of bias."
  },
  {
    "objectID": "index.html#average-treatment-effect-estimation",
    "href": "index.html#average-treatment-effect-estimation",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.5 Average treatment effect estimation",
    "text": "3.5 Average treatment effect estimation\nTo this point, we have been using odds ratios (from logistic regression) in various fancy ways to generate other helpful statistics (probabilities of harm, benefit, etc.).\nBut odds ratios are not infallible. Firstly, there is the issue of noncollapsibility.\nAs Sander Greenland explains:\n\n“Noncollapsibility is a noncausal phenomenon in which a measurement on a group does not equal a designated average of the same measurement over its constituents, as illustrated by how group odds are not simple averages of individual odds when the odds vary across individuals”\n\nBlogs by Cameron Patrick and Solomun Kurz provide excellent overviews of non-collapsiblity and obtaining non-odds ratio estimates from logistic regression. See also Frank Harrell’s comments for a great example of non-collapsbility.\nFor our purposes, the noncollapsbility of ORs means that our marginal OR from DECADE is not a weighted average of possible hidden subpopulations. Another effect of non-collapsiblity is that the effect estimate will change when adjusting for a variable that is not a confounder, while it does not change for collapsible measures (e.g., relative risk).\nAnother issue is that odds ratios are not a true measure of the ‘average treatment effect (ATE)’. Some have also referred to the ATE as the “difference in probabilities”, “discrete differences,” or “discrete change.” The ATE is a population-level summary of the effect of an intervention. A true measure of the ATE is, for example, the risk difference.\n\n3.5.1 ATE table\nSo, let’s get into estimating the ATE, in the form of the risk differences obtained from an ANOVA-type logistic regression model. For this we will use the avg_comparisons() function in the marginaleffects package, which calculates estimates from brms (and frequentist) models.\n\n\n\n\nTable 4: Average treatment effect of dexmedetomidine in the DECADE trial\n\n\n\n\n\n\n\n\n\n\nRisk difference % (DEX - placebo)\n95% CrI\n\n\n\n\nReference priors\n\n\nVague\n5.2\n0.4, 10.0\n\n\nSceptical\n4.2\n0.9, 7.5\n\n\nNeutral\n3.9\n-0.4, 8.1\n\n\nOptimistic\n1.1\n-2.6, 4.6\n\n\nMeta-analysis priors\n\n\nMA (all studies)\n-0.6\n-4.0, 2.7\n\n\nMA (low RoB)\n0.2\n-3.5, 3.8\n\n\nMA (bias adj.)\n3.8\n-0.3, 7.9"
  },
  {
    "objectID": "index.html#combined-plot",
    "href": "index.html#combined-plot",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.6 Combined plot",
    "text": "3.6 Combined plot"
  },
  {
    "objectID": "index.html#forest-plot-all-subgroups",
    "href": "index.html#forest-plot-all-subgroups",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "4.1 Forest plot all subgroups",
    "text": "4.1 Forest plot all subgroups\n\n\n\n\n\n\n\n\nFigure 8: Metaregression plots."
  },
  {
    "objectID": "index.html#explained-variability",
    "href": "index.html#explained-variability",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "4.2 Explained variability",
    "text": "4.2 Explained variability\nNow to look at how much heterogeneity our moderators actually explain. A great overview is provided by Wolfgang Vietchbauer.\nBasically, we can compare the values for \\(\\tau^2\\) in the adjusted and unadjusted model.\nTo calculate the proportion of explained variability (pseudo\\(R^2\\)), we can use the formula:\n\\[\nR^2 = \\frac{\\tau^2 (\\text{unadjusted model}) - \\tau^2 (\\text{adjusted model})}{\\tau^2 (\\text{unadjusted model})}\n\\]\nGiven we have a distribution of posteriors for \\(\\tau\\), rather than an estimate (as they do in frequentist analysis), let’s use the median \\(\\tau\\) from our distribution.\n\n\n\n\nTable 5: Proportion of explain variation by the metaregressors.\n\n\n\n\n\n\n\n\n\n\nMedian τ²\nProportion of R²\n\n\n\n\nOverall\n0.16\nNA\n\n\nDose timing\n0.13\n16.5%\n\n\nControl agent\n0.11\n30.4%"
  },
  {
    "objectID": "index.html#comparing-dose-timing",
    "href": "index.html#comparing-dose-timing",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "7.3 Comparing dose timing",
    "text": "7.3 Comparing dose timing\nHelp for was obtained here.\n\n7.3.1 Figure\n\n\n\n\n\nFigure 14: Comparison of effect with different dose timing.\n\n\n\n\n\n\n7.3.2 Tablulated\nThe includes the (one-sided hypothesis test) evidence ratio, which is described by Paul Buerkner as:\n\n“That is, when the hypothesis is of the form a &gt; b, the evidence ratio is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. In this example, values greater than one indicate that the evidence in favor of a &gt; b is larger than evidence in favor of a &lt; b.”\n\n\n\n\n\n\n\nTable 10:  Comparison of effect with different dose timing. \n  \n    \n    \n      \n      Evidence ratio1\n      Posterior probability\n      Estimate (95% CrI)\n    \n  \n  \n    Intraoperative (any benefit)\n2.44\n0.71\n-0.19 (-0.79 to 0.35)\n    Intraoperative + posteropative (any benefit)\n3.14\n0.76\n-0.23 (-0.78 to 0.26)\n    Postoperative (any benefit)\n169.21\n0.99\n-0.56 (-0.94 to -0.21)\n    Intraoperative + postoperative superior to intraoperative\n1.07\n0.52\n-0.03 (-0.73 to 0.63)\n    Intraoperative + postoperative superior to postoperative\n0.21\n0.17\n0.33 (-0.30 to 0.91)\n    Postoperative superior to intraoperative\n5.41\n0.84\n-0.37 (-0.98 to 0.27)\n  \n  \n  \n    \n      1 This is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. Values &gt;1 are evidence in favour of the specified hypothesis."
  },
  {
    "objectID": "index.html#forest-plot",
    "href": "index.html#forest-plot",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "5.1 Forest plot",
    "text": "5.1 Forest plot\nFor the ‘Mean (SD)’ columns for the continuous outcomes, we used the unweighted grand mean (means of all the studys’ means and SDs). As such, these numbers as essentially arbitrary (because means from tiny studies are weighted equally to means from massive studies). We just present these numbers to give the reader an idea of the sorts of values that were observed in the included studies.\n\n\n\n\n\n\n\n\nFigure 10: Forest plot of the secondary outcomes."
  },
  {
    "objectID": "index.html#probability-of-harm-calculations",
    "href": "index.html#probability-of-harm-calculations",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "5.2 Probability of harm calculations",
    "text": "5.2 Probability of harm calculations\nLet’s also look at the probability of dexmedetomidine causing various levels of harm, in the form of bradycardia and hypotension. This is equal and opposite to Table 1. Table 7 below shows the relevant figures.\n\n\n\n\nTable 7: Probability of harm of dexmedetomidine in causing bradycardia and hypotension, across various subgroups.\n\n\n\n\n\n\n\n\n\n\nMedian control group rate (%)\nProbability of any harm (%)\nProbability of harm &gt;MCID\n\n\n\n\nBradycardia\n\n\nOverall\n7.5\n67.3\n37.1\n\n\nHypotension\n\n\nOverall\n54.2\n35.6\n14.1"
  },
  {
    "objectID": "index.html#probability-of-benefit-calculations-1",
    "href": "index.html#probability-of-benefit-calculations-1",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.3 Probability of benefit calculations",
    "text": "8.3 Probability of benefit calculations\n\n8.3.1 Binary outcomes\nFor two of our dichotomous secondary outcomes, mortality, and arrhythmia, dexmedetomidine may yield benefit. So, below in Table 9, I’ve calculated the probability of various NNTs for these outcomes, again divided into subgroups overall, age and dose.\nMany of the values for mortality are NA. This is because the mortality rate is low (1-2%), so it is impossible to have an absolute risk reduction of &gt;1-2% (you can’t have a negative absolute risk).\n\n\nError in eval(expr, envir, enclos): object 'm.brm_arrhythmia_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_arrhythmia_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_arrhythmia_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_arrhythmia_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_arrhythmia' not found\n\n\nError in names(x) &lt;- value: 'names' attribute [7] must be the same length as the vector [5]\n\n\nError in eval(expr, envir, enclos): object 'm.brm_mortality_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_mortality_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_mortality_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_mortality_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_mortality' not found\n\n\nError: object 'prob_tbl2' not found\n\n\nError in eval(expr, envir, enclos): object 'prob_tbl2' not found\n\n\nError: object 'prob_tbl' not found\n\n\nError in eval(expr, envir, enclos): object 'prob_tbl' not found\n\n\n\n\nTable 9: Probability of benefit of dexmedetomidine in preventing arrhythmias and mortality, across various subgroups.\n\n\n\n\n\n\n\n\n\n\nEvidence ratio1\nPosterior probability\nEstimate (95% CrI)\n\n\n\n\nDose timing\n\n\nIntraoperative (any benefit)\n2.68\n0.73\n-0.20 (-0.76 to 0.33)\n\n\nIntraoperative + posteropative (any benefit)\n3.18\n0.76\n-0.23 (-0.78 to 0.24)\n\n\nPostoperative (any benefit)\n159.00\n0.99\n-0.56 (-0.94 to -0.21)\n\n\nIntraoperative + postoperative superior to intraoperative\n1.06\n0.51\n-0.03 (-0.71 to 0.60)\n\n\nIntraoperative + postoperative superior to postoperative\n0.21\n0.18\n0.33 (-0.31 to 0.90)\n\n\nPostoperative superior to intraoperative\n5.36\n0.84\n-0.36 (-0.97 to 0.24)\n\n\nControl agent\n\n\nMorphine (any benefit)\n11.99\n0.92\n-0.51 (-1.11 to 0.07)\n\n\nPropofol (any benefit)\n144.45\n0.99\n-0.75 (-1.25 to -0.27)\n\n\nNormal saline (any benefit)\n7.75\n0.89\n-0.24 (-0.62 to 0.08)\n\n\nPropofol superior to morphine\n2.58\n0.72\n-0.25 (-0.95 to 0.45)\n\n\nPropofol superior to normal saline\n12.14\n0.92\n-0.51 (-1.09 to 0.09)\n\n\nNormal saline superior to morphine\n0.32\n0.24\n0.27 (-0.40 to 0.92)\n\n\n\n1 This is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. Values &gt;1 are evidence in favour of the specified hypothesis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n8.3.2 Continuous outcomes\nWe also have favourable continuous secondary outcomes. Unlike our binary outcomes, we need to choose specific mean differences that we think would be meaningful. Obviously this would be different for each outcome.\nBefore we decide on this, let’s focus on mean differences on 0.5, 1, and 2. Table 10 shows the results.\n\n\nError in eval(expr, envir, enclos): object 'm.brm_del_duration_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_del_duration_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_del_duration_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_del_duration_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_del_duration' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_extub_time_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_extub_time_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_extub_time_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_extub_time_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_extub_time' not found\n\n\nError: object 'prob_tbl2' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_hosp_stay_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_hosp_stay_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_hosp_stay_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_hosp_stay_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_hosp_stay' not found\n\n\nError: object 'prob_tbl3' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_icu_stay_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_icu_stay_age' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_icu_stay_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'm.brm_icu_stay_dose' not found\n\n\nError in eval(expr, envir, enclos): object 'probs_age_below_60_icu_stay' not found\n\n\nError: object 'prob_tbl4' not found\n\n\nError in eval(expr, envir, enclos): object 'prob_tbl2' not found\n\n\nError: object 'prob_tbl' not found\n\n\nError in eval(expr, envir, enclos): object 'prob_tbl' not found\n\n\n\n\nTable 10: Probability of benefit of dexmedetomidine in decreasing delirium duration, time to extubation, hospital stay, and ICU stay, across various subgroups.\n\n\n\n\n\n\n\n\n\n\nEvidence ratio1\nPosterior probability\nEstimate (95% CrI)\n\n\n\n\nDose timing\n\n\nIntraoperative (any benefit)\n2.68\n0.73\n-0.20 (-0.76 to 0.33)\n\n\nIntraoperative + posteropative (any benefit)\n3.18\n0.76\n-0.23 (-0.78 to 0.24)\n\n\nPostoperative (any benefit)\n159.00\n0.99\n-0.56 (-0.94 to -0.21)\n\n\nIntraoperative + postoperative superior to intraoperative\n1.06\n0.51\n-0.03 (-0.71 to 0.60)\n\n\nIntraoperative + postoperative superior to postoperative\n0.21\n0.18\n0.33 (-0.31 to 0.90)\n\n\nPostoperative superior to intraoperative\n5.36\n0.84\n-0.36 (-0.97 to 0.24)\n\n\nControl agent\n\n\nMorphine (any benefit)\n11.99\n0.92\n-0.51 (-1.11 to 0.07)\n\n\nPropofol (any benefit)\n144.45\n0.99\n-0.75 (-1.25 to -0.27)\n\n\nNormal saline (any benefit)\n7.75\n0.89\n-0.24 (-0.62 to 0.08)\n\n\nPropofol superior to morphine\n2.58\n0.72\n-0.25 (-0.95 to 0.45)\n\n\nPropofol superior to normal saline\n12.14\n0.92\n-0.51 (-1.09 to 0.09)\n\n\nNormal saline superior to morphine\n0.32\n0.24\n0.27 (-0.40 to 0.92)\n\n\n\n1 This is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. Values &gt;1 are evidence in favour of the specified hypothesis."
  },
  {
    "objectID": "index.html#bayesian-regression-test",
    "href": "index.html#bayesian-regression-test",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.1 Bayesian regression test",
    "text": "6.1 Bayesian regression test\nArguably the most popular method of sample study effect assessment is through analysis of funnel plot asymmetry, in the form of Egger’s regression test (and variants therein). This test involves regressing the effect sizes of studies according to their precision; slopes that are significantly different from 0 suggest sample sample effects. This can also be applied in a Bayesian framework, and was described recently by Shi et al. The interpretation regarding the slope is similar to the frequentist Egger’s test. Shi et al. use the latent “true” SEs in the Egger-type regression under the Bayesian framework.\nLike many sample bias tests, this actually assesses funnel plot asymmetry (of which publication bias is only 1 cause).\nFor a list of alternative causes of funnel plot asymmetry, see Harrer et al.\nBriefly, they are:\n\nBetween-study effect heterogeneity\nHeterogeneity in types of feasible methods of small vs. large studies\nSmall studies being more poorly run than large studies\nRandom change\n\nThere are various options for inputs. We will use multiplicative heterogeneity rather than additive heterogeneity. We will use a half-normal prior with 0.5 scale parameter (Cauchy distributions are not available but the difference is likely trivial). The default vague priors for the regression intercept and slope are used: \\(N(0, 10^4)\\), as this is the approach use for the model’s validation in Shi et al.\nAs you can see in Table 8, the 95%CrI for the slope excludes 0, whether looking at all studies or excluding studies at high risk of bias. This suggests the presence of publication bias."
  },
  {
    "objectID": "index.html#all-studies-6",
    "href": "index.html#all-studies-6",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.2 All studies",
    "text": "9.2 All studies\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 29\n   Total graph size: 277\n\nInitializing model\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 30\n   Unobserved stochastic nodes: 25\n   Total graph size: 233\n\nInitializing model\n\n\n\n\n\n\nTable 14:  Bayesian regression testing to assess for small study effects. \n  \n    \n    \n      Estimate\n      95% credible interval\n    \n  \n  \n    \n      All studies\n    \n    3.86\n2.41, 5.50\n    \n      Excluding studies at high risk of bias\n    \n    3.52\n1.83, 5.28"
  },
  {
    "objectID": "index.html#bayesian-model-averaging",
    "href": "index.html#bayesian-model-averaging",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.3 Bayesian model averaging",
    "text": "6.3 Bayesian model averaging\nBayesian model averaging is well described by Bartos et al.:\n\n“In practice, researchers seldom have knowledge about the data-generating process nor do they have sufficient information to choose with confidence among the wide variety of proposed methods that aim to adjust for publication bias. Furthermore, this wide range of proposed methods often leads to contradictory conclusions. The combination of uncertainty about the data-generating process and the presence of conflicting conclusions can create a”breeding ground” for confirmation bias: researchers may unintentionally select those methods that support the desired outcome. This freedom to choose can greatly inflate the rate of false positives, which can be a serious problem for conventional meta-analysis methods.”\n\nThe RoBMA packages generates three families of models across 12 models (interfacing with JAGS through rjags), and averages their performance. A total of 32 of these models include publication bias and 4 do not. For a description see this link. The output includes an inclusion Bayes factor. The Bayes factor is the ratio of two marginal likelihoods (i.e., two probabilities of data given certain models). We will produce Bayes factors for the mean effect, heterogeneity, and publication bias.\nThe “inclusion” part of “inclusion Bayes factor” refers to the fact we are comparing the alternative hypothesis relative to the null hypothesis. Hence our notation for the bayes factor is \\(BF_{10}\\) rather than \\(BF_{01}\\), which is another kind of Bayes factor that compares the null hypothesis relative to the alternative hypothesis. So, for the mean effect, our null model is \\(H_0: {\\mu} = 0\\) and alterative model is \\(H_1: {\\mu} ≠ 0\\). Then, the inclusion Bayes factor is:\n\\[\nBF_{10} = \\frac{p(data | H_1)}{p(data | H_0)}\n\\] Bayes factors answers the question: Are the observed data more probable under models with a particular effect, than they are under models without that particular effect? Inclusion Bayes factors &gt;3 indicate ‘substantial’ evidence against the null, while inclusion Bayes factors &lt;1/3 indicate ‘substantial’ evidence for the null.\nThe benefit provided by the model averaging approach is that we are provided with an inclusion Bayes factor for publication bias. A high inclusion BF for publication bias would suggest that models accounting for publication bias are more consistent with the observed data. We are also provided with an estimate that accounts for publication bias.\nPublication bias models include selection models and PET-PEESE models as methods of publication bias assessment. The publication bias adjustment prior is described in Bartoš (2021).\nTable 9 shows the results.\n\n6.3.1 Summary\n\n\n\n\nTable 9: Results of the model-averaging approach to meta-analysis, with results for models including publication bias and excluding publication bias.\n\n\n\n\n\n\n\n\n\n\nNumber of models\nPrior probability\nPosterior probability\nInclusion Bayes factor\n\n\n\n\nModels accounting for publication bias\n\n\nEffect\n18\n0.50\n0.37\n0.58\n\n\nHeterogeneity\n18\n0.50\n0.44\n0.77\n\n\nPublication bias\n32\n0.50\n1.00\n214.18\n\n\nModels not accounting for publication bias\n\n\nEffect\n2\n0.50\n0.79\n3.73\n\n\nHeterogeneity\n2\n0.50\n0.92\n10.97\n\n\nPublication bias\n0\n0.00\n0.00\n0.00\n\n\n\n\n\n\n\n\n\n\n\n\n6.3.2 Estimates\nBelow we present the model-averaged estimates from the robust Bayesian meta-analysis. We present model-averaged estimates (averaged across models assuming the null is true, and models assuming the alternative hypothesis is true) as well as conditional estimates (averaged over only models that assume the alternative hypothesis is true).\n\n\n\n\n\n\n\n\n\nMean\nMedian\n2.5th percentile\n97.5th percentile\n\n\n\n\nAveraged across all models\n\n\nμ1\n1.15\n1.00\n0.93\n2.51\n\n\nτ2\n0.09\n0.00\n0.00\n0.45\n\n\nAveraged across models assuming the alternative hypothesis is true\n\n\nμ\n1.45\n1.37\n0.82\n2.88\n\n\nτ\n0.21\n0.19\n0.01\n0.53\n\n\n\n1 Estimates are odds ratios\n\n\n2 Estimates are log odds ratios\n\n\n\n\n\n\n\n\n\n\n6.3.3 Plots\nNow let’s plot the posterior distributions for the mean effect to visualise the effect of publication bias. In Figure 11 below, (A) shows the distribution of the model-averaged estimate with models including publication bias, and (B) is the average distribution of models that do not include publication bias.\nOur model-averaged estimates in these plots use models that assume the absence of an effect (other options includes models that assume the presence of an effect).\nThe arrows demonstrate the probability of a spike at \\(log(OR) = 0\\) (no effect).\nEvidently, including publication bias models significantly changes the results.\n\n\n\n\n\n\n\n\nFigure 11: Plots of the model-averaged mean effect when considering (A) all models including those accounting for publication bias, and (B) models excluding those accounting for publication bias."
  },
  {
    "objectID": "index.html#frequentist-methods-for-small-study-effects",
    "href": "index.html#frequentist-methods-for-small-study-effects",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "8.4 Frequentist methods for small study effects",
    "text": "8.4 Frequentist methods for small study effects\n\n8.4.1 Contour-enhanced funnel plot\nBelow in Figure 13 is the contour-enhanced funnel plot. As you can see, there is clear asymmetry to the plot, with small studies missing from the right hand side (area of no effect or harm).\nWe are actually using the meta package for frequentist analysis here (this is the most aesthetically pleasing option for contour-enhanced funnel plots in my opinion), rather than metafor. However the results are the same as the methods I’ve used are identical in both packages.\n\n\n\n\n\n\n\n\nFigure 13: Contour-enhaced funnel plot for the outcome of postoperative delirium.\n\n\n\n\n\n\n\n8.4.2 Henmi & Copas method\nRandom effects meta-analysis is not always better than fixed effect meta-analysis, even when you are certain that your population of trials do not meet the (almost invariably un-meet-able) requirement for a fixed effect meta-analysis, which is that the variability of population effects in your trials should be 0. Once instance where random effects falls down is in the allocation of weight to included trials in the presence of heterogeneity in observed effect sizes.\nFor a random effects inverse variance meta-analysis, each study is weighed according to its variance and the calculated heterogeneity:\n\\[\nw_i = \\frac{1}{v_i + \\tau}\n\\tag{7}\\]\nFor a fixed effect analysis, \\(\\tau\\) is not estimated, and the weight of studies is given by:\n\\[\nw_i = \\frac{1}{v_i}\n\\tag{8}\\]\nIndeed, this is partly why the random effects inverse variance meta-analysis converges to a fixed effect analysis when \\(\\tau\\) is estimated to be 0.\nHowever, in the presence of significant heterogeneity in random effects meta-analysis, Equation 7 results in the relative downweighting of larger studies and upweighting of smaller studies. This means smaller studies have a greater influence on the pooled outcome, which exacerbates any impact of publication bias.\nHenmi and Copas proposed that studies in a random effects meta-analysis be weighted according to fixed effect Equation 8. This means that \\(\\tau\\) is calculated and incorporated into the results but is not used to weight the studies. This approach was later popularised by Doi et al. as the ‘Inverse Variance Heterogeneity Model’.\nTable 11 shows the results.\n\n\n\n\nTable 11: Results of the Henmi and Copas method for random effects meta-analysis, where studies are weighed according using fixed effect weighting.\n\n\n\n\n\n\n\n\n\nOdds ratio\n95% confidence interval\n95% prediction interval\n\n\n\n\n0.79\n0.51, 1.22\n0.29, 2.12\n\n\n\n\n\n\n\n\n\n\n\n\n8.4.3 Selection models\nSelection models are another form of publication bias assessment. These revolve around testing the hypothesis that there is a systematic ‘selection’ of low p-values relative to higher p-values. They also provide revised effect sizes that take into account the hypothesized selection model.\nWe have used the step function to create the so-called “three-parameter selection model”. The parameters are \\(\\mu\\) (mean effect size), \\({\\tau}^2\\) (heterogeneity variance), and \\(d^2\\) (likelihood of selection of p-values). We can’t really do any more complex models because we only have 16 studies.\nSee the bookdown selection model page and the metafor selection model page, and Dan Quintana’s blog for more details.\n\n\n\nRandom-Effects Model (k = 12; tau^2 estimator: ML)\n\ntau^2 (estimated amount of total heterogeneity): 0.1839 (SE = 0.1990)\ntau (square root of estimated tau^2 value):      0.4289\n\nTest for Heterogeneity:\nLRT(df = 1) = 2.4951, p-val = 0.1142\n\nModel Results:\n\nestimate      se     zval    pval    ci.lb   ci.ub    \n -0.5455  0.4240  -1.2864  0.1983  -1.3765  0.2856    \n\nTest for Selection Model Parameters:\nLRT(df = 1) = 0.1238, p-val = 0.7250\n\nSelection Model Results:\n\n                    k  estimate      se    zval    pval   ci.lb   ci.ub    \n0     &lt; p &lt;= 0.025  3    1.0000     ---     ---     ---     ---     ---    \n0.025 &lt; p &lt;= 1      9    1.6967  2.4920  0.2796  0.7798  0.0000  6.5809    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n8.4.4 Trim and fill\nNow for trim and fill analysis. This is another method of publication bias assessment that has been around a lot longer than the other examples.\nTrim and fill analysis uses the funnel plot to hypothesise the existence of ‘missing’ studies in the plot - i.e., studies that were completed but then put in the ‘file draw’ and not published because they didn’t show a significant result. It also provides a revised effect size accounting for these missing studies.\n\nFunnel\nFirst let’s look at the funnel plot to see how many of these theorised studies there are and where they may sit.\n\n\n\n\n\n\n\n\n\n\n\nRevised effect size\nNow let’s print the revised effect size from the trim and fill analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOdds ratio\n95% confidence interval\n95% prediction interval\n\n\n\n\n0.84\n0.60, 1.16\n0.29, 2.40"
  },
  {
    "objectID": "index.html#comparing-subgroups",
    "href": "index.html#comparing-subgroups",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "4.3 Comparing subgroups",
    "text": "4.3 Comparing subgroups\nThe idea for this analysis was obtained here.\nWe will conduct hypothesis tests to compare the levels of the factor in the multilevel model.\n\n4.3.1 Figure\nLet’s start by plotting the posterior distributions for each of the variables’ levels.\n\n\n\n\n\n\n\n\nFigure 9: Comparison of effect with different dose timing.\n\n\n\n\n\n\n\n4.3.2 Tablulated\nNow let’s use the brms function brms::hypothesis() to compare levels of the variables.\nThe includes the (one-sided hypothesis test) evidence ratio, which is described by Paul Buerkner as:\n\n“That is, when the hypothesis is of the form a &gt; b, the evidence ratio is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. In this example, values greater than one indicate that the evidence in favor of a &gt; b is larger than evidence in favor of a &lt; b.”\n\n\n\n\n\nTable 6: Comparison of effect with different dose timing.\n\n\n\n\n\n\n\n\n\n\nEvidence ratio1\nPosterior probability\nEstimate (95% CrI)\n\n\n\n\nDose timing\n\n\nIntraoperative (any benefit)\n2.40\n0.71\n-0.18 (-0.75 to 0.36)\n\n\nIntraoperative + posteropative (any benefit)\n3.35\n0.77\n-0.22 (-0.75 to 0.24)\n\n\nPostoperative (any benefit)\n180.82\n0.99\n-0.56 (-0.94 to -0.21)\n\n\nIntraoperative + postoperative superior to intraoperative\n1.14\n0.53\n-0.04 (-0.73 to 0.60)\n\n\nIntraoperative + postoperative superior to postoperative\n0.20\n0.17\n0.33 (-0.27 to 0.90)\n\n\nPostoperative superior to intraoperative\n5.47\n0.85\n-0.37 (-1.00 to 0.25)\n\n\nControl agent\n\n\nMorphine (any benefit)\n10.25\n0.91\n-0.50 (-1.11 to 0.12)\n\n\nPropofol (any benefit)\n136.93\n0.99\n-0.76 (-1.26 to -0.27)\n\n\nNormal saline (any benefit)\n8.22\n0.89\n-0.23 (-0.59 to 0.07)\n\n\nPropofol superior to morphine\n2.57\n0.72\n-0.26 (-0.99 to 0.44)\n\n\nPropofol superior to normal saline\n12.89\n0.93\n-0.53 (-1.09 to 0.07)\n\n\nNormal saline superior to morphine\n0.33\n0.25\n0.27 (-0.43 to 0.93)\n\n\n\n1 This is the ratio of the posterior probability of a &gt; b and the posterior probability of a &lt; b. Values &gt;1 are evidence in favour of the specified hypothesis."
  },
  {
    "objectID": "index.html#all-studies-3",
    "href": "index.html#all-studies-3",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "9.2 All studies",
    "text": "9.2 All studies\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 29\n   Total graph size: 277\n\nInitializing model\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 30\n   Unobserved stochastic nodes: 25\n   Total graph size: 233\n\nInitializing model\n\n\n\n\nTable 9: Bayesian regression testing to assess for small study effects.\n\n\n\n\n\n\n\n\n\nEstimate\n95% credible interval\n\n\n\n\nAll studies\n\n\n3.86\n2.41, 5.50\n\n\nExcluding studies at high risk of bias\n\n\n3.52\n1.83, 5.28"
  },
  {
    "objectID": "index.html#priors-explanations",
    "href": "index.html#priors-explanations",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "3.2 Priors explanations",
    "text": "3.2 Priors explanations\n\n\n\n\n\n\n\n\nPrior\nPrior equivalent\nRationale for prior\n\n\n\n\nReference priors\n\n\nVague\nNo information imposed on posterior estimate\nDoes not favour one prior belief over another\n\n\nSceptical\nEquivalent to a hypothetical n = 1000 RCT showing a 31% increase in odds of delirium\nThis effect size is 1.5 times the MCID for harm (logOR of 0.27)\n\n\nNeutral\n95% of the density lies between an odds ratio of 0.5 to 2.0\nPlausible values for the effect are likely, with values closer to the null most likely\n\n\nOptimistic\nEquivalent to a hypothetical n = 1000 RCT showing a 24% decrease in odds of delirium\nThis effect size is the 1.5 times MCID for benefit (logOR of -0.27)\n\n\nMeta-analysis priors\n\n\nMA (all studies)\nMeta-analysis of n = 2745 participants across 11 trials\nThis is analogous to the result of a standard meta-analysis of all 12 studies, including the DECADE trial\n\n\nMA (low RoB)\nMeta-analysis of n = 787 participants across 3 trials\nThis attempts to attenuate the effects of various research bias parameters but using only high-quality studies\n\n\nMA (bias adj.)\nMeta-analysis of n = 2745 participants across 11 trials averaged across models accounting for publication bias\nThis attempts to attenuate the influence of publication bias in the literature"
  },
  {
    "objectID": "index.html#all-studies-2",
    "href": "index.html#all-studies-2",
    "title": "Dexmedetomidine in cardiac surgery meta-analysis",
    "section": "6.2 All studies",
    "text": "6.2 All studies\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 36\n   Unobserved stochastic nodes: 29\n   Total graph size: 277\n\nInitializing model\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 30\n   Unobserved stochastic nodes: 25\n   Total graph size: 233\n\nInitializing model\n\n\n\n\nTable 8: Bayesian regression testing to assess for small study effects.\n\n\n\n\n\n\n\n\n\nEstimate\n95% credible interval\n\n\n\n\nAll studies\n\n\n3.86\n2.41, 5.50\n\n\nExcluding studies at high risk of bias\n\n\n3.52\n1.83, 5.28"
  }
]